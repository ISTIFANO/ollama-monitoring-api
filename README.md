# ğŸš€ Ollama Monitoring API - Projet MLOps

API de monitoring complÃ¨te pour **Ollama** (modÃ¨le **Qwen-8b quantifiÃ©**) avec **FastAPI**, **Prometheus**, **Grafana** et **cAdvisor**.  
Architecture optimisÃ©e pour **6GB RAM / 2 CPU cores** avec monitoring avancÃ© des ressources.

---

## ğŸ“Š Architecture du Projet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OLLAMA MONITORING API                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Ollama  â”‚â—„â”€â”€â”€â”€â–ºâ”‚ FastAPI  â”‚      â”‚ cAdvisor â”‚        â”‚
â”‚  â”‚  qwen2.5 â”‚      â”‚   API    â”‚      â”‚ Metrics  â”‚        â”‚
â”‚  â”‚ (4GB/2CPU)â”‚      â”‚          â”‚      â”‚          â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â”‚                 â”‚                  â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                         â”‚                                   â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                  â”‚  Prometheus   â”‚                         â”‚
â”‚                  â”‚  Time-Series  â”‚                         â”‚
â”‚                  â”‚     DB        â”‚                         â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                         â”‚                                   â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                  â”‚    Grafana    â”‚                         â”‚
â”‚                  â”‚  Dashboards   â”‚                         â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Structure ComplÃ¨te du Projet

```
â”œâ”€â”€ docker/                                # Configuration Docker
â”‚   â”œâ”€â”€ docker-compose.yml                 # Orchestration complÃ¨te
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ prometheus.yml                 # Config Prometheus + cAdvisor
â”‚       â””â”€â”€ grafana/
â”‚           â””â”€â”€ provisioning/
â”‚               â”œâ”€â”€ datasources/           # Auto-config Prometheus
â”‚               â”‚   â””â”€â”€ datasource.yml
â”‚               â””â”€â”€ dashboards/            # Dashboards auto-importÃ©s
â”‚                   â”œâ”€â”€ dashboard.yml
â”‚                   â””â”€â”€ ollama-dashboard.json
â”‚
â”œâ”€â”€ api/                                   # Application FastAPI
â”‚   â”œâ”€â”€ Dockerfile                         
â”‚   â”œâ”€â”€ requirements.txt                   
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py                        # Point d'entrÃ©e + /metrics
â”‚       â”œâ”€â”€ config.py                      # Config avec optimisations RAM
â”‚       â”œâ”€â”€ metrics.py                     # MÃ©triques Prometheus custom
â”‚       â”œâ”€â”€ routers/
â”‚       â”‚   â”œâ”€â”€ health.py                  # Healthcheck API + Ollama
â”‚       â”‚   â””â”€â”€ chat.py                    # Endpoint /chat avec mÃ©triques
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â””â”€â”€ ollama_client.py           # Client HTTP Ollama
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ retry.py                   # Retry logic avec backoff
â”‚           â””â”€â”€ timers.py                  # Mesure latence prÃ©cise
â”‚
â”œâ”€â”€ scripts/                               # Scripts DevOps
â”‚   â”œâ”€â”€ setup_model.py                     # Installation modÃ¨le optimisÃ©
â”‚   â”œâ”€â”€ stress_test.py                     # Test de charge
â”‚   â””â”€â”€ warmup.py                          # PrÃ©-chargement modÃ¨le
â”‚
â”œâ”€â”€ .env                                   # Variables d'environnement
â”œâ”€â”€ README.md                              # Cette documentation
â””â”€â”€ Makefile                               # Commandes pratiques
```

---

## ğŸ¯ FonctionnalitÃ©s MLOps

### âœ… Monitoring Complet
- **RAM Usage** : Suivi en temps rÃ©el de la consommation mÃ©moire du container Ollama
- **CPU Usage** : Monitoring de l'utilisation CPU avec alertes de seuil
- **OOM Events** : DÃ©tection des Ã©vÃ©nements Out-Of-Memory
- **Latence API** : Mesure des temps de rÃ©ponse (p50, p95, p99)
- **Request Rate** : Taux de requÃªtes (succÃ¨s vs erreurs)
- **Network I/O** : Bande passante rÃ©seau du container

### ğŸ”§ Optimisations pour 4GB RAM
- Limites strictes Docker: `mem_limit: 4G`, `cpus: 2.0`
- ModÃ¨le quantifiÃ© recommandÃ©: `qwen2.5:0.5b` (500MB)
- Variables d'environnement optimisÃ©es:
  - `OLLAMA_NUM_PARALLEL=1` : Un seul modÃ¨le en mÃ©moire
  - `OLLAMA_MAX_LOADED_MODELS=1` : Ã‰vite le multi-modÃ¨le
  - `OLLAMA_FLASH_ATTENTION=1` : Optimisation mÃ©moire attention

### ğŸ“¡ Exposition des MÃ©triques
- **FastAPI** `/metrics` : MÃ©triques applicatives (requests, latency, errors)
- **cAdvisor** `:8080/metrics` : MÃ©triques containers (RAM, CPU, I/O, OOM)
- **Prometheus** `:9090` : AgrÃ©gation et stockage time-series
- **Grafana** `:3000` : Dashboards visuels (login: admin/admin)

---

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis
- Docker & Docker Compose
- **10GB RAM minimum** sur la machine hÃ´te (6GB pour Ollama + overhead)
- Python 3.11+ (pour scripts hors Docker)

### Ã‰tape 1 : DÃ©marrer les Services

```bash
# Construire et dÃ©marrer tous les containers
make up

# Alternative sans Makefile
cd docker
docker-compose up -d
```

**Services disponibles :**
- API FastAPI : http://localhost:8000
- Documentation API : http://localhost:8000/docs
- cAdvisor : http://localhost:8080
- Prometheus : http://localhost:9090
- Grafana : http://localhost:3000 (admin/admin)

### Ã‰tape 2 : Installer le ModÃ¨le Qwen-8b

```bash
# Attendre que le container Ollama soit dÃ©marrÃ© (30-40 secondes)
docker-compose logs -f ollama

# Installer le modÃ¨le qwen-8b quantifiÃ© (recommandÃ©)
python scripts/setup_model.py
```

**Options de modÃ¨les pour 6GB RAM (qwen-8b quantifiÃ©s) :**
| ModÃ¨le | Taille | Quantization | Description |
|--------|--------|--------------|-------------|
| `qwen2.5:7b-instruct-q4_0` | 4.4GB | q4_0 | âœ… **RecommandÃ©** - Bon Ã©quilibre |
| `qwen2.5:7b-instruct-q4_K_M` | 4.7GB | q4_K_M | âœ… Meilleure qualitÃ© |
| `qwen2.5:7b-instruct-q5_K_M` | 5.4GB | q5_K_M | âœ… QualitÃ© maximale |

> **Note:** qwen2.5:7b-instruct correspond au modÃ¨le qwen-8b avec diffÃ©rents niveaux de quantization.

### Ã‰tape 3 : VÃ©rifier le Monitoring

```bash
# Tester l'API
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain quantum computing in one sentence"}'

# AccÃ©der au dashboard Grafana
open http://localhost:3000
# Login: admin / admin
# Dashboard: "Ollama Monitoring - MLOps Dashboard"
```

---

## ğŸ“Š Dashboard Grafana

Le dashboard inclut automatiquement :

### ğŸ“ˆ Panels Disponibles
1. **Memory Usage** - Consommation RAM vs limite 4GB
2. **CPU Usage** - Utilisation CPU en pourcentage
3. **OOM Events** - Compteur d'Ã©vÃ©nements Out-Of-Memory
4. **Memory Usage %** - Gauge RAM en %
5. **Total Requests** - Nombre total de requÃªtes API
6. **API Latency (p95)** - Latence 95Ã¨me percentile
7. **API Response Time** - Temps de rÃ©ponse p50/p95/p99
8. **Request Rate** - Taux de requÃªtes succÃ¨s vs erreurs
9. **Network I/O** - Bande passante rÃ©seau

### ğŸ¨ Visualisation
- Refresh automatique toutes les 10 secondes
- PÃ©riode par dÃ©faut : 1 heure
- Tags : `ollama`, `monitoring`, `mlops`

---

## ğŸ” MÃ©triques Prometheus Disponibles

### MÃ©triques cAdvisor (Container)
```promql
# Utilisation mÃ©moire
container_memory_usage_bytes{name="ollama"}

# Limite mÃ©moire
container_spec_memory_limit_bytes{name="ollama"}

# Utilisation CPU
rate(container_cpu_usage_seconds_total{name="ollama"}[1m])

# Ã‰vÃ©nements OOM
container_oom_events_total{name="ollama"}

# RÃ©seau I/O
rate(container_network_receive_bytes_total{name="ollama"}[1m])
rate(container_network_transmit_bytes_total{name="ollama"}[1m])
```

### MÃ©triques FastAPI Custom
```promql
# RequÃªtes totales
ollama_requests_total{method="POST", endpoint="/chat", status="success"}

# Latence (histogram)
histogram_quantile(0.95, rate(ollama_request_duration_seconds_bucket[5m]))

# RequÃªtes actives
ollama_active_requests

# Erreurs
ollama_errors_total{error_type="TimeoutError"}
```

---

## ğŸ› ï¸ Commandes Utiles (Makefile)

```bash
make help       # Afficher toutes les commandes disponibles
make build      # Construire les containers
make up         # DÃ©marrer tous les services
make down       # ArrÃªter tous les services
make logs       # Suivre les logs en temps rÃ©el
make restart    # RedÃ©marrer les services
make clean      # Nettoyer complÃ¨tement (volumes + images)
make stress     # Lancer un test de charge
make warmup     # PrÃ©chauffer le modÃ¨le
```

---

## ğŸ’¡ Conseils d'Optimisation MÃ©moire
Configuration Optimale pour qwen-8b (6GB RAM)

#### 1. ModÃ¨le RecommandÃ©
```bash
# âœ… RecommandÃ© pour 6GB - qwen-8b quantifiÃ©
qwen2.5:7b-instruct-q4_0   # 4.4GB - Meilleur Ã©quilibre
qwen2.5:7b-instruct-q4_K_M # 4.7GB - QualitÃ© supÃ©rieure
qwen2.5:7b-instruct-q5_K_M # 5.4GB - QualitÃ© maximale
```

#### 2. Variables d'environnement Docker
```yaml
# docker-compose.yml
deploy:
  resources:
    limits:
      cpus: '2.0'
      memory: 6G      # 6GB pour qwen-8b
    reservations:
      memory: 3G
      
environment:
  - OLLAMA_NUM_PARALLEL=1        # Limite Ã  1 modÃ¨le simultanÃ©
  - OLLAMA_MAX_LOADED_MODELS=1   # Pas de multi-modÃ¨le
  - OLLAMA_FLASH_ATTENTION=1     # Optimise l'attention mechanism
```

#### 3. Configuration API
```python
# config.py
MAX_CONTEXT_LENGTH=4096  # Contexte adaptÃ© pour qwen-8b
ENABLE_STREAMING=true    # Streaming disponible avec 6GB
ENABLE_STREAMING=false   # DÃ©sactive streaming (rÃ©duit charge)
```

#### 4. Rate Limiting
```bash
# Limiter les requÃªtes concurrentes
MAX_REQUESTS_PER_MINUTE=60
```

### ğŸ“‰ Monitoring Proactif

#### Alertes Prometheus (Ã  ajouter)
```yaml
# alerts.yml
- alert: HighMemoryUsage
  expr: (container_memory_usage_bytes{name="ollama"} / container_spec_memory_limit_bytes{name="ollama"}) > 0.9
  for: 2m
  annotations:
    summary: "Ollama utilise >90% de la RAM"

- alert: OOMDetected
  expr: increase(container_oom_events_total{name="ollama"}[5m]) > 0
  annotations:
    summary: "OOM Kill dÃ©tectÃ© sur Ollama!"
```

### ğŸ”„ Warmup du ModÃ¨le

```bash
# Charger le modÃ¨le en mÃ©moire avant production
python scripts/warmup.py

# Cela Ã©vite la latence du premier appel
```

---

## ğŸ§ª Tests et Validation

### Test de Charge
```bash
# Test avec 50 requÃªtes, concurrence de 5
python scripts/stress_test.py

# Surveiller Grafana pendant le test pour observer:
# - Pic de CPU
# - Augmentation RAM
# - Latence API
```

### Test Manuel
```bash
# Healthcheck
curl http://localhost:8000/health
curl http://localhost:8000/health/ollama

# Chat simple
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello!"}'

# Chat avec modÃ¨le spÃ©cifique
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain AI", "model": "qwen2.5:0.5b"}'
```

---

## ğŸ”§ DÃ©pannage

### Container Ollama ne dÃ©marre pas
```bash
# VÃ©rifier les logs
docker-compose logs ollama

# VÃ©rVÃ©rifier que vous utilisez bien qwen-8b quantifiÃ©
OLLAMA_MODEL=qwen2.5:7b-instruct-q4_0

# 2. Augmenter lÃ©gÃ¨rement la limite si nÃ©cessaire
mem_limit: 6G  # DÃ©jÃ  configurÃ© pour qwen-8b

# 3. Surveiller l'utilisation dans Grafana
# Dashboard > Memory Usage devrait rester < 5.5GB5b

# 2. Augmenter la limite (si possible)
mem_limit: 6G

# 3. RÃ©duire le contexte
MAX_CONTEXT_LENGTH=1024
```

### Latence API Ã©levÃ©e
```bash
# 1. PrÃ©chauffer le modÃ¨le
python scripts/warmup.py

# 2. VÃ©rifier l'utilisation CPU/RAM dans Grafana

# 3. RÃ©duire la concurrence
MAX_REQUESTS_PER_MINUTE=30
```

### Grafana dashboard vide
```bash
# VÃ©rifier que Prometheus scrape correctement
curl http://localhost:9090/api/v1/targets

# VÃ©rifier que cAdvisor fonctionne
curl http://localhost:8080/metrics
```

---

## ğŸ“š Ressources ComplÃ©mentaires

### Documentation Officielle
- [Ollama Documentation](https://github.com/ollama/ollama)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [cAdvisor GitHub](https://github.com/google/cadvisor)

### ModÃ¨les Qwen
- [Qwen2.5 Model Card](https://huggingface.co/Qwen)
- Quantization formats: q4_0, q4_K_M, q5_K_M

---

## ğŸ—ï¸ Architecture MLOps Best Practices

### âœ… ImplÃ©mentÃ©
- âœ… SÃ©paration des responsabilitÃ©s (API, Model, Monitoring)
- âœ… Conteneurisation complÃ¨te avec Docker
- âœ… ObservabilitÃ© avec mÃ©triques Prometheus
- âœ… Dashboards Grafana pour visualisation
- âœ… Healthchecks et retry logic
- âœ… Configuration via variables d'environnement
- âœ… Resource limits (CPU/RAM)
- âœ… Auto-provisioning Grafana

### ğŸ”œ AmÃ©liorations Futures
- [ ] Alerting avec Alertmanager
- [ ] CI/CD avec GitHub Actions
- [ ] Tests unitaires et d'intÃ©gration
- [ ] Load balancing avec Nginx
- [ ] Logging centralisÃ© (ELK/Loki)
- [ ] Secrets management (Vault)
- [ ] A/B testing de modÃ¨les
- [ ] Model versioning

---

## ğŸ“ Configuration DÃ©taillÃ©e
7b-instruct-q4_0  # qwen-8b quantifiÃ©

# === API Configuration ===
API_TIMEOUT=300                         # Timeout en secondes
MAX_RETRIES=3                           # Nombre de retry
RETRY_DELAY=1                           # DÃ©lai entre retries (s)

# === Rate Limiting ===
MAX_REQUESTS_PER_MINUTE=60

# === Memory Optimization (6GB RAM) ===
MAX_CONTEXT_LENGTH=4096                 # Contexte adaptÃ© pour qwen-8b
ENABLE_STREAMING=true                   # Streaming disponible avec 6GB
# === Rate Limiting ===
MAX_REQUESTS_PER_MINUTE=60

# === Memory Optimization ===
MAX_CONTEXT_LENGTH=2048              # Limite contexte (tokens)
ENABLE_STREAMING=false               # DÃ©sactive streaming
```

### Docker Compose Resources
6G       # 6GB pour qwen-8b
    reservations:
      cpus: '1.0'      # Minimum garanti
      memory: 3
    limits:
      cpus: '2.0'      # Maximum 2 CPU cores
      memory: 4G       # Maximum 4GB RAM
    reservations:
      cpus: '1.0'      # Minimum garanti
      memory: 2G       # RAM minimum garantie
```

---

## ğŸ¤ Contribution

Les contributions sont bienvenues! Pour contribuer:

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

---

## ğŸ“„ Licence

Ce projet est sous licence MIT.

---

## ğŸ‘¤ Auteur

**Ollama Monitoring API** - Projet MLOps/DevOps pour monitoring de modÃ¨les LLM

---

## ğŸ¯ RÃ©sumÃ© Rapide

```bash
# 1. DÃ©marrer la stack
make up

# 2. Installer le modÃ¨le
python scripts/setup_model.py

# 3. Tester l'API
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello world!"}'

# 4. Ouvrir Grafana
open http://localhost:3000  # admin/admin
Ce projet utilise **qwen-8b quantifiÃ© en q4** (qwen2.5:7b-instruct-q4_0) avec **6GB de RAM**. Pour des contraintes
# 5. Profit! ğŸ‰
```

---

**Note importante** : Pour utiliser le vrai modÃ¨le **qwen-8b en q4**, vous aurez besoin d'au moins **5-6GB de RAM**. Pour rester dans la limite de 4GB, utilisez `qwen2.5:0.5b`, `qwen2.5:1.5b`, ou `qwen2.5:3b`.
