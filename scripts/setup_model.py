#!/usr/bin/env python3
"""
Script pour t√©l√©charger et optimiser le mod√®le Ollama
G√®re le t√©l√©chargement du mod√®le avec v√©rification de la m√©moire disponible
"""
import asyncio
import httpx
import sys


MODELS_INFO = {
    "qwen2.5:0.5b": {"size_gb": 0.5, "recommended": False, "description": "Petit mod√®le pour contraintes extr√™mes"},
    "qwen2.5:1.5b": {"size_gb": 1.0, "recommended": False, "description": "Mod√®le l√©ger"},
    "qwen2.5:3b": {"size_gb": 1.9, "recommended": False, "description": "Mod√®le interm√©diaire"},
    "qwen2.5:7b-instruct-q4_0": {"size_gb": 4.4, "recommended": True, "description": "qwen-8b quantifi√© q4_0 - Recommand√© pour 6GB"},
    "qwen2.5:7b-instruct-q4_K_M": {"size_gb": 4.7, "recommended": True, "description": "qwen-8b quantifi√© q4_K_M - Qualit√© sup√©rieure"},
    "qwen2.5:7b-instruct-q5_K_M": {"size_gb": 5.4, "recommended": True, "description": "qwen-8b quantifi√© q5 - Meilleure qualit√©"},
}


async def check_ollama_service(url: str = "http://localhost:11434"):
    """V√©rifier que le service Ollama est disponible"""
    print(f"üîç V√©rification du service Ollama sur {url}...")
    async with httpx.AsyncClient(timeout=10.0) as client:
        try:
            response = await client.get(f"{url}/api/tags")
            if response.status_code == 200:
                print("‚úÖ Service Ollama disponible")
                return True
            else:
                print(f"‚ùå Service Ollama r√©pond avec le code: {response.status_code}")
                return False
        except Exception as e:
            print(f"‚ùå Impossible de se connecter √† Ollama: {str(e)}")
            print("üí° Assurez-vous que le container Ollama est d√©marr√©: docker-compose up -d ollama")
            return False


async def list_models(url: str = "http://localhost:11434"):
    """Lister les mod√®les d√©j√† install√©s"""
    async with httpx.AsyncClient(timeout=10.0) as client:
        try:
            response = await client.get(f"{url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                if models:
                    print("\nüì¶ Mod√®les d√©j√† install√©s:")
                    for model in models:
                        name = model.get("name", "Unknown")
                        size = model.get("size", 0) / (1024**3)  # Convert to GB
                        print(f"  - {name} ({size:.2f} GB)")
                else:
                    print("\nüì¶ Aucun mod√®le install√©")
                return models
        except Exception as e:
            print(f"‚ùå Erreur lors de la liste des mod√®les: {str(e)}")
            return []


async def pull_model(model_name: str, url: str = "http://localhost:11434"):
    """T√©l√©charger un mod√®le Ollama"""
    print(f"\nüì• T√©l√©chargement du mod√®le: {model_name}")
    
    # Afficher les infos du mod√®le si disponibles
    if model_name in MODELS_INFO:
        info = MODELS_INFO[model_name]
        print(f"   Taille approximative: {info['size_gb']} GB")
        print(f"   Description: {info['description']}")
        if not info['recommended']:
            print("   ‚ö†Ô∏è  ATTENTION: Ce mod√®le peut causer des probl√®mes avec 4GB RAM")
            response = input("   Voulez-vous continuer? (oui/non): ")
            if response.lower() not in ['oui', 'o', 'yes', 'y']:
                print("   ‚ùå T√©l√©chargement annul√©")
                return False
    
    async with httpx.AsyncClient(timeout=600.0) as client:
        try:
            # Pull request avec streaming
            async with client.stream(
                'POST',
                f"{url}/api/pull",
                json={"name": model_name}
            ) as response:
                if response.status_code != 200:
                    print(f"‚ùå Erreur: {response.status_code}")
                    return False
                
                print("   Progression:")
                async for line in response.aiter_lines():
                    if line:
                        import json
                        try:
                            data = json.loads(line)
                            status = data.get("status", "")
                            if "completed" in data and "total" in data:
                                completed = data["completed"] / (1024**2)
                                total = data["total"] / (1024**2)
                                percent = (data["completed"] / data["total"]) * 100
                                print(f"   {status}: {completed:.1f}/{total:.1f} MB ({percent:.1f}%)", end='\r')
                            else:
                                print(f"   {status}")
                        except json.JSONDecodeError:
                            pass
                
                print("\n‚úÖ Mod√®le t√©l√©charg√© avec succ√®s!")
                return True
                
        except Exception as e:
            print(f"\n‚ùå Erreur lors du t√©l√©chargement: {str(e)}")
            return False


async def verify_model(model_name: str, url: str = "http://localhost:11434"):
    """V√©rifier qu'un mod√®le fonctionne correctement"""
    print(f"\nüß™ Test du mod√®le {model_name}...")
    
    async with httpx.AsyncClient(timeout=120.0) as client:
        try:
            response = await client.post(
                f"{url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": "Hello!",
                    "stream": False
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                print(f"‚úÖ Mod√®le fonctionnel!")
                print(f"   R√©ponse: {data.get('response', '')[:100]}...")
                return True
            else:
                print(f"‚ùå Erreur: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur lors du test: {str(e)}")
            return False


def print_recommendations():
    """Afficher les recommandations pour l'utilisation avec 6GB RAM"""
    print("\n" + "="*60)
    print("üìã RECOMMANDATIONS POUR 6GB RAM")
    print("="*60)
    print("\n‚úÖ Mod√®les recommand√©s (qwen-8b quantifi√©s):")
    for model_name, info in MODELS_INFO.items():
        if info['recommended']:
            print(f"  ‚Ä¢ {model_name} - {info['size_gb']}GB")
            print(f"    {info['description']}")
    
    print("\n‚ö†Ô∏è  Conseils d'optimisation:")
    print("  1. Utiliser OLLAMA_NUM_PARALLEL=1 (un seul mod√®le √† la fois)")
    print("  2. Contexte adapt√©: MAX_CONTEXT_LENGTH=4096")
    print("  3. Streaming activ√© disponible avec 6GB")
    print("  4. Surveiller les m√©triques avec Grafana")
    print("  5. Configurer des alertes si RAM >90%")
    
    print("\nüîß Variables d'environnement importantes:")
    print("  OLLAMA_NUM_PARALLEL=1")
    print("  OLLAMA_MAX_LOADED_MODELS=1")
    print("  OLLAMA_FLASH_ATTENTION=1")
    print("="*60 + "\n")


async def main():
    """Point d'entr√©e principal"""
    print("="*60)
    print("üöÄ OLLAMA MODEL SETUP - Optimis√© pour 6GB RAM")
    print("="*60)
    
    url = "http://localhost:11434"
    
    # V√©rifier le service
    if not await check_ollama_service(url):
        sys.exit(1)
    
    # Lister les mod√®les existants
    await list_models(url)
    
    # Afficher les recommandations
    print_recommendations()
    
    # Demander quel mod√®le t√©l√©charger
    print("\nüí° Mod√®le par d√©faut recommand√©: qwen2.5:7b-instruct-q4_0 (qwen-8b)")
    model_choice = input("Entrez le nom du mod√®le √† t√©l√©charger (ou Enter pour qwen2.5:7b-instruct-q4_0): ").strip()
    
    if not model_choice:
        model_choice = "qwen2.5:7b-instruct-q4_0"
    
    # T√©l√©charger le mod√®le
    success = await pull_model(model_choice, url)
    
    if success:
        # V√©rifier le mod√®le
        await verify_model(model_choice, url)
        
        print(f"\n‚úÖ Configuration termin√©e!")
        print(f"   Mod√®le install√©: {model_choice}")
        print(f"   Mettez √† jour le fichier .env avec: OLLAMA_MODEL={model_choice}")
    else:
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
